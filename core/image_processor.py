# ======================== ğŸ“¦ æ¨¡çµ„èˆ‡ä¾è³´ ========================
import cv2
import time
import torch
import numpy as np
from PIL import Image
from functools import lru_cache
import torchvision.transforms as transforms
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights

from utils.logger import logger
from utils.gpu_utils import gpu_manager

# ======================== âš™ï¸ å…¨åŸŸè®Šæ•¸ ========================
_image_model = None
_transform = None
_model_loaded = False
_feature_cache = {}

# ======================== ğŸ§¹ è³‡æºç®¡ç† ========================
def cleanup():
    """æ¸…ç† GPU è³‡æº"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gpu_manager.clear_gpu_memory()

# =============== ğŸ§  ç‰¹å¾µæ“·å–ï¼šæ„ŸçŸ¥å“ˆå¸Œï¼ˆpHashï¼‰ ===============
@lru_cache(maxsize=1024)
def compute_phash(image_path: str) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:
    """è¨ˆç®—ç°åº¦ã€é‚Šç·£èˆ‡é¡è‰²ä¸‰ç¨®ç‰¹å¾µçš„æ„ŸçŸ¥å“ˆå¸Œ"""
    try:
        img = cv2.imread(image_path)
        if img is None:
            return None

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        gray = cv2.GaussianBlur(gray, (5, 5), 1)
        gray = cv2.resize(gray, (64, 64))

        edges = cv2.Canny(gray, 50, 150)
        edges = cv2.resize(edges, (64, 64))

        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        hsv = cv2.resize(hsv, (64, 64))

        if torch.cuda.is_available():
            gray_tensor = torch.from_numpy(gray).float().cuda()
            edge_tensor = torch.from_numpy(edges).float().cuda()
            hsv_tensor = torch.from_numpy(hsv[:, :, :2]).float().cuda()

            gray_dct = torch.fft.rfft2(gray_tensor)
            gray_dct_low = torch.abs(gray_dct[:32, :32])
            gray_threshold = torch.mean(gray_dct_low.cpu()) + 0.3 * torch.std(gray_dct_low.cpu())
            gray_hash = (gray_dct_low > gray_threshold.cuda()).cpu().numpy()

            edge_dct = torch.fft.rfft2(edge_tensor)
            edge_dct_low = torch.abs(edge_dct[:32, :32])
            edge_mean = torch.mean(edge_dct_low.cpu())
            edge_hash = (edge_dct_low > edge_mean.cuda()).cpu().numpy()

            hsv_mean = torch.mean(hsv_tensor, dim=(0, 1))
            hsv_hash = (hsv_tensor > hsv_mean.reshape(1, 1, -1)).cpu().numpy()
        else:
            gray_dct = cv2.dct(np.float32(gray))[:32, :32]
            gray_hash = gray_dct > np.mean(gray_dct) + 0.3 * np.std(gray_dct)

            edge_dct = cv2.dct(np.float32(edges))[:32, :32]
            edge_hash = edge_dct > np.mean(edge_dct)

            hsv_mean = np.mean(hsv[:, :, :2], axis=(0, 1))
            hsv_hash = hsv[:, :, :2] > hsv_mean.reshape(1, 1, -1)

        return gray_hash, edge_hash, hsv_hash

    except Exception as e:
        logger.error(f"è¨ˆç®—å¤šé‡ç‰¹å¾µå“ˆå¸Œæ™‚å‡ºéŒ¯ {image_path}: {str(e)}")
        return None

# =============== ğŸ§  ç‰¹å¾µæ“·å–ï¼šæ·±åº¦æ¨¡å‹ï¼ˆMobileNetV3ï¼‰ ===============
def get_image_model():
    """è¼‰å…¥ MobileNetV3-Large æ¨¡å‹èˆ‡å‰è™•ç†æµç¨‹"""
    global _image_model, _transform, _model_loaded
    if not _model_loaded:
        try:
            start_time = time.time()
            logger.info("é–‹å§‹è¼‰å…¥ MobileNetV3-Large æ¨¡å‹...")
            _image_model = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)
            _image_model = torch.nn.Sequential(*list(_image_model.children())[:-1])
            if torch.cuda.is_available():
                _image_model = _image_model.cuda()
            _image_model.eval()

            _transform = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
            ])
            _model_loaded = True
            logger.info(f"æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œè€—æ™‚: {time.time() - start_time:.2f}ç§’")
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¨¡å‹æ™‚å‡ºéŒ¯: {str(e)}")
            raise
    return _image_model, _transform

def compute_batch_embeddings(image_paths: List[str], batch_size: int = 64) -> Optional[np.ndarray]:
    """æ‰¹æ¬¡è¨ˆç®—åœ–åƒæ·±åº¦åµŒå…¥å‘é‡"""
    try:
        model, transform = get_image_model()
        embeddings = []

        with ThreadPoolExecutor() as executor:
            futures = [
                executor.submit(lambda p: transform(Image.open(p).convert('RGB')), path)
                for path in image_paths if path not in _feature_cache
            ]
            for path in image_paths:
                if path in _feature_cache:
                    embeddings.append(_feature_cache[path])

            batch_tensors = []
            for future in futures:
                try:
                    batch_tensors.append(future.result())
                except Exception as e:
                    logger.error(f"è™•ç†åœ–åƒæ™‚å‡ºéŒ¯: {str(e)}")

        if batch_tensors:
            for i in range(0, len(batch_tensors), batch_size):
                batch = torch.stack(batch_tensors[i:i + batch_size])
                if torch.cuda.is_available():
                    batch = batch.cuda()
                with torch.no_grad():
                    features = model(batch).cpu()
                for j, f in enumerate(features):
                    vec = f.squeeze().numpy()
                    embeddings.append(vec)
                    _feature_cache[image_paths[i + j]] = vec

                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

        return np.array(embeddings) if embeddings else None

    except Exception as e:
        logger.error(f"æ‰¹æ¬¡è¨ˆç®—åµŒå…¥å‘é‡æ™‚å‡ºéŒ¯: {str(e)}")
        return None

# =============== ğŸ§ª ç‰¹å¾µæ¯”å°é‚è¼¯ ===============
def fast_similarity(feat1: Tuple[np.ndarray, np.ndarray, np.ndarray],
                    feat2: Tuple[np.ndarray, np.ndarray, np.ndarray]) -> float:
    """å¿«é€Ÿæ¯”å° pHash ç‰¹å¾µçš„ç¶œåˆç›¸ä¼¼åº¦"""
    if all(f1 is not None and f2 is not None for f1, f2 in zip(feat1, feat2)):
        gray_sim = 1 - np.count_nonzero(feat1[0] != feat2[0]) / feat1[0].size
        edge_sim = 1 - np.count_nonzero(feat1[1] != feat2[1]) / feat1[1].size
        hsv_sim = 1 - np.count_nonzero(feat1[2] != feat2[2]) / feat1[2].size
        return gray_sim * 0.5 + edge_sim * 0.3 + hsv_sim * 0.2
    return 0

# =============== ğŸ¬ è¦–é »ç›¸ä¼¼åº¦æ¯”å°ä¸»æµç¨‹ ===============
def video_similarity(frames1: List[str], frames2: List[str],
                     video_duration: float,
                     batch_size: int = 64) -> Dict[str, float]:
    """å…©éšæ®µè¦–è¨Šç›¸ä¼¼åº¦æ¯”å°æµç¨‹"""
    try:
        # å‹•æ…‹èª¿æ•´æ¡æ¨£èˆ‡é–€æª»
        if video_duration <= 60:
            sample_interval, phash_threshold = 1, 0.6
        elif video_duration <= 300:
            sample_interval, phash_threshold = 2, 0.65
        else:
            sample_interval, phash_threshold = 3, 0.7

        sampled_frames1 = frames1[::sample_interval]
        sampled_frames2 = frames2[::sample_interval]

        logger.info(f"æ¡æ¨£å¹€æ•¸: è¦–é »1={len(sampled_frames1)}ï¼Œè¦–é »2={len(sampled_frames2)}")

        # ====== ç¬¬ä¸€éšæ®µï¼špHash å¿«é€Ÿæ¯”å° ======
        with ThreadPoolExecutor() as executor:
            phash1 = list(executor.map(compute_phash, sampled_frames1))
            phash2 = list(executor.map(compute_phash, sampled_frames2))

        valid1 = [(f, p) for f, p in zip(sampled_frames1, phash1) if p is not None]
        valid2 = [(f, p) for f, p in zip(sampled_frames2, phash2) if p is not None]

        if not valid1 or not valid2:
            return {"similarity": 0.0}

        similar_pairs = []
        phash_similarities = []
        for f1, p1 in valid1:
            max_sim = 0
            best = None
            for f2, p2 in valid2:
                sim = fast_similarity(p1, p2)
                if sim > max_sim:
                    max_sim = sim
                    best = (f2, sim)
            phash_similarities.append(max_sim)
            if best and best[1] >= phash_threshold:
                similar_pairs.append((f1, best[0]))

        avg_phash = np.mean(phash_similarities)
        matched_ratio = len(similar_pairs) / len(valid1)

        if not similar_pairs:
            return {
                "similarity": avg_phash,
                "filtered_similarity": 0.0,
                "similar_pairs": 0,
                "total_pairs": len(valid1),
                "phash_threshold": phash_threshold
            }

        # ====== ç¬¬äºŒéšæ®µï¼šæ·±åº¦ç‰¹å¾µæ¯”å° ======
        e1 = compute_batch_embeddings([p[0] for p in similar_pairs], batch_size)
        e2 = compute_batch_embeddings([p[1] for p in similar_pairs], batch_size)

        if e1 is None or e2 is None:
            return {"similarity": 0.0}

        e1 /= np.linalg.norm(e1, axis=1, keepdims=True) + 1e-8
        e2 /= np.linalg.norm(e2, axis=1, keepdims=True) + 1e-8

        sim_matrix = np.dot(e1, e2.T)
        deep_sim = float(np.mean(np.max(sim_matrix, axis=1)))

        weight_factor = min(1.0, matched_ratio / 0.3)
        final_sim = weight_factor * (matched_ratio * deep_sim + (1 - matched_ratio) * avg_phash)

        return {
            "similarity": final_sim,
            "deep_similarity": deep_sim,
            "phash_similarity": avg_phash,
            "similar_pairs": len(similar_pairs),
            "total_pairs": len(valid1),
            "phash_threshold": phash_threshold
        }

    except Exception as e:
        logger.error(f"è¨ˆç®—è¦–é »ç›¸ä¼¼åº¦æ™‚å‡ºéŒ¯: {str(e)}")
        return {"similarity": 0.0}
